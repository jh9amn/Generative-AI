{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c229b174-05b4-45ce-9696-4e3f705636ce",
   "metadata": {},
   "source": [
    "# 🧠 Word Embedding in NLP\n",
    "\n",
    "## 📖 Introduction\n",
    "\n",
    "In **Natural Language Processing (NLP)**, a **word embedding** is a technique that represents words as numerical vectors.  \n",
    "Instead of treating words as discrete symbols, embeddings capture the **semantic meaning** — words with similar meanings end up having **similar vector representations** in a continuous vector space.\n",
    "\n",
    "In simple terms:\n",
    "> **Word Embedding = Representation of words as numerical vectors that capture meaning and relationships.**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Why Do We Need Word Embedding?\n",
    "\n",
    "Computers can’t understand text directly — they work with numbers.  \n",
    "Traditional text representations (like one-hot encoding) assign a unique index to each word, but this has limitations:\n",
    "- It doesn’t capture **meaning** or **context**.\n",
    "- It produces **sparse**, high-dimensional vectors.\n",
    "- It can’t handle **semantic similarity** between words (e.g., *king* and *queen* are unrelated numerically).\n",
    "\n",
    "Word embeddings solve these issues by mapping words to **dense, low-dimensional vectors** that reflect their meanings.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Word Embedding Approaches\n",
    "\n",
    "There are two major categories:\n",
    "\n",
    "### 1️⃣ **Count or Frequency-Based Methods**\n",
    "\n",
    "These are **statistical** techniques that analyze word occurrence frequency in documents.\n",
    "\n",
    "#### a) 🟩 One-Hot Encoding\n",
    "\n",
    "- Each word is represented as a **binary vector**.\n",
    "- If a word exists in the vocabulary, its position is marked as `1`, otherwise `0`.\n",
    "\n",
    "Example:\n",
    "\n",
    "| Word  | Pizza | Burger | Pasta |\n",
    "|--------|--------|--------|--------|\n",
    "| Pizza  | 1 | 0 | 0 |\n",
    "| Burger | 0 | 1 | 0 |\n",
    "| Pasta  | 0 | 0 | 1 |\n",
    "\n",
    "**Advantages:**\n",
    "- Simple and easy to implement.\n",
    "\n",
    "**Disadvantages:**\n",
    "- **No semantic meaning captured** (e.g., \"dog\" and \"cat\" are equally distant).\n",
    "- **Sparse vectors** → leads to inefficient computation.\n",
    "- Vocabulary grows rapidly → large dimensionality.\n",
    "\n",
    "---\n",
    "\n",
    "#### b) 🟨 Bag of Words (BoW)\n",
    "\n",
    "- Represents text as the **frequency of words** in a document.\n",
    "- Order of words is **ignored**.\n",
    "\n",
    "Example:\n",
    "\n",
    "> Sentence 1: “I love pizza”  \n",
    "> Sentence 2: “I love burger”\n",
    "\n",
    "Vocabulary = [I, love, pizza, burger]\n",
    "\n",
    "| Sentence | I | love | pizza | burger |\n",
    "|-----------|---|------|--------|---------|\n",
    "| 1 | 1 | 1 | 1 | 0 |\n",
    "| 2 | 1 | 1 | 0 | 1 |\n",
    "\n",
    "**Advantages:**\n",
    "- Easy to create and interpret.\n",
    "- Works well for simple models.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Ignores word order and context.\n",
    "- Produces sparse and high-dimensional data.\n",
    "- No semantic meaning captured.\n",
    "\n",
    "---\n",
    "\n",
    "#### c) 🟦 TF-IDF (Term Frequency – Inverse Document Frequency)\n",
    "\n",
    "TF-IDF improves upon BoW by considering the **importance** of a word.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "\\[\n",
    "\\text{TF-IDF} = \\text{TF} \\times \\text{IDF}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- **TF (Term Frequency)** = Frequency of a word in a document.  \n",
    "  \\[\n",
    "  TF = \\frac{\\text{No. of times word appears in a document}}{\\text{Total words in the document}}\n",
    "  \\]\n",
    "- **IDF (Inverse Document Frequency)** = Importance of the word across documents.  \n",
    "  \\[\n",
    "  IDF = \\log_e\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing the word}}\\right)\n",
    "  \\]\n",
    "\n",
    "**Advantages:**\n",
    "- Reduces the weight of common words like “the”, “is”, etc.\n",
    "- Highlights important and rare words.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Still sparse and ignores semantic relationships.\n",
    "- Cannot capture context or word order.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ **Deep Learning-Based Models (Neural Embeddings)**\n",
    "\n",
    "Unlike count-based models, these use **neural networks** to learn embeddings that capture **semantic and syntactic relationships**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🌐 Word2Vec\n",
    "\n",
    "Developed by **Google (Mikolov et al., 2013)**, Word2Vec converts words into dense vectors based on their **context** in sentences.\n",
    "\n",
    "It uses a **shallow neural network** to learn embeddings in one of two ways:\n",
    "\n",
    "---\n",
    "\n",
    "##### a) 🧩 CBOW (Continuous Bag of Words)\n",
    "\n",
    "**Goal:** Predict the **target word** using the **context words**.\n",
    "\n",
    "Example:  \n",
    "> “The cat sat on the ___.”\n",
    "\n",
    "CBOW tries to predict the missing word “mat” from the surrounding words [\"The\", \"cat\", \"sat\", \"on\", \"the\"].\n",
    "\n",
    "**Advantages:**\n",
    "- Faster and efficient for large datasets.\n",
    "- Works well for frequent words.\n",
    "\n",
    "---\n",
    "\n",
    "##### b) ⚙️ Skip-Gram\n",
    "\n",
    "**Goal:** Predict **context words** given a **target word**.\n",
    "\n",
    "Example:  \n",
    "> For the target word “cat”, predict nearby words like [\"The\", \"sat\", \"on\"].\n",
    "\n",
    "**Advantages:**\n",
    "- Performs better for rare words.\n",
    "- Captures complex word relationships.\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Visualization of Word2Vec\n",
    "\n",
    "Word embeddings map words into vector space where **semantic relationships** are preserved.\n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "\n",
    "vector(\"King\") - vector(\"Man\") + vector(\"Woman\") ≈ vector(\"Queen\")\n",
    "\n",
    "```\n",
    "\n",
    "This means the model understands **gender and relational context** between words.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 Summary Table\n",
    "\n",
    "| Method | Type | Captures Meaning | Handles Context | Dimensionality | Example |\n",
    "|---------|------|------------------|-----------------|----------------|----------|\n",
    "| One-Hot Encoding | Count-Based | ❌ | ❌ | High | [1, 0, 0, 0, ...] |\n",
    "| Bag of Words | Count-Based | ❌ | ❌ | High | [2, 1, 0, ...] |\n",
    "| TF-IDF | Count-Based | ⚠️ Partial | ❌ | High | [0.45, 0.00, 0.87, ...] |\n",
    "| Word2Vec (CBOW / SkipGram) | Neural | ✅ | ✅ | Low | [0.23, -0.12, 0.54, ...] |\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Conclusion\n",
    "\n",
    "Word embeddings have revolutionized NLP by providing a **numerical way to represent text meaningfully**.  \n",
    "They form the foundation for modern NLP tasks like:\n",
    "- Sentiment Analysis  \n",
    "- Machine Translation  \n",
    "- Text Classification  \n",
    "- Chatbots & Question Answering  \n",
    "\n",
    "As NLP evolves, embeddings have become even more powerful through **contextual models** like **BERT**, **GPT**, and **ELMo**, which capture the **meaning of words based on context** in sentences.\n",
    "\n",
    "---\n",
    "\n",
    "⭐ **In summary:**\n",
    "> Word embeddings bridge the gap between human language and machine understanding — transforming words into meaningful mathematical representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
