{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f650a65a-26a8-44e7-ad25-5daabfeb6b2f",
   "metadata": {},
   "source": [
    "## **📊 One Hot Encoding:**\n",
    "\n",
    "| Document | Text              |\n",
    "|-----------|------------------|\n",
    "| D1 | The food is good |\n",
    "| D2 | The food is bad |\n",
    "| D3 | Pizza is amazing |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 Vocabulary (Unique Words)\n",
    "\n",
    "['The', 'food', 'is', 'good', 'bad', 'Pizza', 'amazing']\n",
    "\n",
    "| The | food | is | good | bad | Pizza | amazing |\n",
    "|-----|------|----|------|-----|--------|----------|\n",
    "|  1  |  0   | 0  |  0   | 0   |  0     | 0        |\n",
    "|  0  |  1   | 0  |  0   | 0   |  0     | 0        |\n",
    "|  0  |  0   | 1  |  0   | 0   |  0     | 0        |\n",
    "|  0  |  0   | 0  |  1   | 0   |  0     | 0        |\n",
    "|  0  |  0   | 0  |  0   | 1   |  0     | 0        |\n",
    "|  0  |  0   | 0  |  0   | 0   |  1     | 0        |\n",
    "|  0  |  0   | 0  |  0   | 0   |  0     | 1        |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 One-Hot Encoding Representation\n",
    "\n",
    "### 📄 D1: “The food is good”\n",
    "\n",
    "| Word | The | food | is | good | bad | Pizza | amazing |\n",
    "|------|-----|------|----|------|-----|--------|----------|\n",
    "|      | 1   | 0    | 0  | 0    | 0   | 0      | 0        |\n",
    "|      | 0   | 1    | 0  | 0    | 0   | 0      | 0        |\n",
    "|      | 0   | 0    | 1  | 0    | 0   | 0      | 0        |\n",
    "|      | 0   | 0    | 0  | 1    | 0   | 0      | 0        |\n",
    "\n",
    "**Shape:** `4 × 7`\n",
    "\n",
    "---\n",
    "\n",
    "### 📄 D2: “The food is bad”\n",
    "\n",
    "| Word | The | food | is | good | bad | Pizza | amazing |\n",
    "|------|-----|------|----|------|-----|--------|----------|\n",
    "|      | 1   | 0    | 0  | 0    | 0   | 0      | 0        |\n",
    "|      | 0   | 1    | 0  | 0    | 0   | 0      | 0        |\n",
    "|      | 0   | 0    | 1  | 0    | 0   | 0      | 0        |\n",
    "|      | 0   | 0    | 0  | 0    | 1   | 0      | 0        |\n",
    "\n",
    "**Shape:** `4 × 7`\n",
    "\n",
    "---\n",
    "\n",
    "### 📄 D3: “Pizza is amazing”\n",
    "\n",
    "| Word | The | food | is | good | bad | Pizza | amazing |\n",
    "|------|-----|------|----|------|-----|--------|----------|\n",
    "|      | 0   | 0    | 0  | 0    | 0   | 1      | 0        |\n",
    "|      | 0   | 0    | 1  | 0    | 0   | 0      | 0        |\n",
    "|      | 0   | 0    | 0  | 0    | 0   | 0      | 1        |\n",
    "\n",
    "**Shape:** `3 × 7`\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Summary\n",
    "\n",
    "- Total Vocabulary Size: **7**\n",
    "- One-hot vector length per word = **7**\n",
    "- Each document is represented as a **matrix** where:  \n",
    "  ➤ **Rows = Words in document**  \n",
    "  ➤ **Columns = Vocabulary terms**\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Insight\n",
    "\n",
    "> One-Hot Encoding helps represent each word as a binary vector based on the vocabulary.  \n",
    "> However, it becomes **inefficient for large datasets** — hence modern NLP models use embeddings like **Word2Vec, GloVe, and BERT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a2cd75-0e91-4f1e-b0e0-81ffcee80a6a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ⚙️ How It Works\n",
    "Each unique word in the vocabulary is represented as a **binary vector**.  \n",
    "- The length of each vector equals the total number of words in the vocabulary.\n",
    "- A `1` indicates the presence of the word at that index, and `0` indicates absence.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Advantages\n",
    "\n",
    "1. **Easy to Implement** — works well with Python libraries such as  \n",
    "   → `sklearn.preprocessing.OneHotEncoder`, `pandas.get_dummies()`\n",
    "\n",
    "2. **Simple Representation** — no complex math required.\n",
    "\n",
    "---\n",
    "\n",
    "## ❌ Disadvantages\n",
    "\n",
    "1. **Sparse Matrix Problem** — results in high-dimensional vectors with many zeros, leading to **overfitting**.  \n",
    "2. **Fixed Input Size** — machine learning models require all vectors to have the same length.  \n",
    "3. **No Semantic Understanding** — words like *“good”* and *“great”* are treated as completely unrelated.  \n",
    "4. **OOV Issue (Out-of-Vocabulary)** — fails when a new word appears that wasn’t in the training vocabulary.\n",
    "\n",
    "> 🧠 **Example:**  \n",
    "> If the vocabulary = [Food, Pizza, Burger],  \n",
    "> then the new word *\"Fries\"* cannot be encoded because it doesn’t exist in the predefined vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Summary\n",
    "\n",
    "- **Type:** Categorical Text Representation  \n",
    "- **Use Case:** Small datasets or quick prototyping  \n",
    "- **Limitations:** Doesn’t scale well for large corpora  \n",
    "- **Better Alternatives:** TF-IDF, Word2Vec, GloVe, BERT\n",
    "\n",
    "---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab21f3d-5533-4dbc-90ef-4fb645ddb59e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
