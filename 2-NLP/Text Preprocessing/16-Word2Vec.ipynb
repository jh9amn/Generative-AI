{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6362698e-fddd-4805-a63a-aa7f2e1cb8f5",
   "metadata": {},
   "source": [
    "# 🌐 Word2Vec — Word Embedding Using Neural Networks\n",
    "\n",
    "## 📖 Introduction\n",
    "\n",
    "**Word2Vec** is a powerful technique for natural language processing developed by **Tomas Mikolov and his team at Google in 2013** for learning **word embeddings** — vector representations of words that capture **semantic meaning and relationships**.\n",
    "\n",
    "The **Word2Vec** alogorithm uses a neural synonymous words or suggest additional words for a **partial sentence**. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector.\n",
    "\n",
    "It is a **shallow, two-layer neural network** that takes a large corpus of text as input and produces a **vector space** (typically 100–300 dimensions), where each unique word is represented by a corresponding vector.\n",
    "\n",
    "In this space:\n",
    "- Words that **share common contexts** in the corpus are located **close to one another**.\n",
    "- Similar words (e.g., *car* and *vehicle*) have similar vector representations.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Intuition Behind Word2Vec\n",
    "\n",
    "Traditional models like **Bag of Words (BoW)** or **TF-IDF** represent words as independent features — they **don’t capture relationships** between words.\n",
    "\n",
    "Word2Vec overcomes this by learning **from context**:\n",
    "> “You shall know a word by the company it keeps.” — *J.R. Firth (1957)*\n",
    "\n",
    "In other words, the **meaning of a word** can be derived from its **surrounding words (context)**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Working Principle\n",
    "\n",
    "Word2Vec learns embeddings by **predicting words based on their context** (or vice versa).  \n",
    "There are two main model architectures:\n",
    "\n",
    "### 1️⃣ Continuous Bag of Words (CBOW)\n",
    "\n",
    "**Goal:** Predict the **target word** given its **context words**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧩 Example\n",
    "\n",
    "> Sentence: “The cat sat on the mat”\n",
    "\n",
    "If we choose a **window size of 2**, then for the target word **\"sat\"**, the context words are:\n",
    "```\n",
    "\n",
    "[\"The\", \"cat\", \"on\", \"the\"]\n",
    "\n",
    "```\n",
    "\n",
    "The model tries to **predict “sat”** based on these surrounding words.\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚙️ How It Works\n",
    "\n",
    "1. Input: Context words  \n",
    "2. Output: Target word  \n",
    "3. The model averages the context word vectors to predict the target.  \n",
    "4. During training, weights are adjusted so that words appearing in similar contexts have similar vectors.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧮 Example in Action\n",
    "\n",
    "| Context Words | Target Word |\n",
    "|----------------|-------------|\n",
    "| [\"The\", \"cat\", \"on\", \"the\"] | sat |\n",
    "| [\"cat\", \"sat\", \"the\", \"mat\"] | on |\n",
    "\n",
    "Over time, the network learns that **“cat”**, **“mat”**, and **“sat”** appear together frequently — so their embeddings move closer in vector space.\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚡ Advantages\n",
    "- Fast to train.\n",
    "- Performs well for frequent words.\n",
    "\n",
    "#### ⚠️ Disadvantages\n",
    "- Doesn’t perform as well on rare words (low frequency).\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Skip-Gram Model\n",
    "\n",
    "**Goal:** Predict the **context words** given a **target word**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧩 Example\n",
    "\n",
    "> Sentence: “The cat sat on the mat”\n",
    "\n",
    "For the target word **“sat”** with a window size of 2:\n",
    "```\n",
    "\n",
    "Context Words → [\"The\", \"cat\", \"on\", \"the\"]\n",
    "\n",
    "```\n",
    "\n",
    "The model tries to predict each of these from the target \"sat\":\n",
    "```\n",
    "\n",
    "Input: \"sat\"\n",
    "Output: [\"The\", \"cat\", \"on\", \"the\"]\n",
    "\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚙️ How It Works\n",
    "\n",
    "1. The model takes one **target word** as input.\n",
    "2. It predicts all possible **context words** within the window.\n",
    "3. The neural network adjusts weights to maximize the probability of correct predictions.\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚡ Advantages\n",
    "- Works well with **small datasets**.\n",
    "- Captures **rare words** better than CBOW.\n",
    "\n",
    "#### ⚠️ Disadvantages\n",
    "- Training is **slower** for large corpora.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 Word2Vec Architecture (Simplified)\n",
    "\n",
    "Word2Vec consists of **three main layers**:\n",
    "\n",
    "1. **Input Layer**  \n",
    "   - Represents one-hot encoded target or context word.\n",
    "\n",
    "2. **Hidden Layer**  \n",
    "   - A linear layer with no activation function.\n",
    "   - Learns word embeddings — each word gets a dense vector representation.\n",
    "\n",
    "3. **Output Layer**  \n",
    "   - Predicts the target or context words using **softmax** probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Mathematical Representation\n",
    "\n",
    "Let:\n",
    "- \\( V \\) = Vocabulary size  \n",
    "- \\( N \\) = Embedding vector dimension  \n",
    "\n",
    "Each word \\( w \\) is represented as a **one-hot vector** of size \\( V \\).\n",
    "\n",
    "### Hidden Layer:\n",
    "\\[\n",
    "h = W^T \\times x\n",
    "\\]\n",
    "where \\( W \\) is the weight matrix of size \\( V \\times N \\), and \\( x \\) is the one-hot vector.\n",
    "\n",
    "### Output Layer:\n",
    "\\[\n",
    "u = W'^T \\times h\n",
    "\\]\n",
    "and the softmax function gives the probability distribution over the vocabulary:\n",
    "\\[\n",
    "P(w_t | w_c) = \\frac{e^{u_t}}{\\sum_{j=1}^{V} e^{u_j}}\n",
    "\\]\n",
    "\n",
    "The model learns the **weights (W, W′)** — which become the **word embeddings**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔎 Example of Learned Relationships\n",
    "\n",
    "Word2Vec captures **semantic and syntactic** relationships:\n",
    "\n",
    "| Relationship | Example | Result |\n",
    "|---------------|----------|--------|\n",
    "| Gender | King - Man + Woman | ≈ Queen |\n",
    "| Verb tense | Walk - Walking + Swam | ≈ Swim |\n",
    "| Country–Capital | France - Paris + Italy | ≈ Rome |\n",
    "| Singular–Plural | Cat - Cats + Dogs | ≈ Dog |\n",
    "\n",
    "📘 **Meaning:** Word2Vec understands the relationship “King is to Man as Queen is to Woman” **mathematically!**\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Key Features\n",
    "\n",
    "✅ Captures **semantic** (meaning-based) and **syntactic** (grammar-based) relationships.  \n",
    "✅ Produces **dense** low-dimensional vectors.  \n",
    "✅ Improves performance in **downstream NLP tasks** like:\n",
    "- Sentiment Analysis\n",
    "- Named Entity Recognition\n",
    "- Machine Translation\n",
    "- Chatbots and Q&A\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 Implementation Example (with Gensim in Python)\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"rug\"],\n",
    "    [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model (Skip-Gram)\n",
    "model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, sg=1)\n",
    "\n",
    "# Display the vector for a word\n",
    "print(model.wv['cat'])\n",
    "\n",
    "# Find most similar words\n",
    "print(model.wv.most_similar('cat'))\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Visualization (Conceptual)\n",
    "\n",
    "```\n",
    "         ┌─────────────────────────────┐\n",
    "         │         Word2Vec Model       │\n",
    "         ├─────────────────────────────┤\n",
    "         │ 1️⃣ Input Word  →  One-Hot    │\n",
    "         │ 2️⃣ Hidden Layer → Embedding  │\n",
    "         │ 3️⃣ Output → Context Prediction│\n",
    "         └─────────────────────────────┘\n",
    "\n",
    "Example:\n",
    "Input: “King”\n",
    "Output Predictions: [“Queen”, “Prince”, “Monarch”, “Royalty”, …]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Summary — CBOW vs Skip-Gram\n",
    "\n",
    "| Feature       | CBOW                       | Skip-Gram                  |\n",
    "| ------------- | -------------------------- | -------------------------- |\n",
    "| Input         | Context Words              | Target Word                |\n",
    "| Output        | Target Word                | Context Words              |\n",
    "| Speed         | Faster                     | Slower                     |\n",
    "| Rare Words    | Poor                       | Better                     |\n",
    "| Training Data | Large Corpus               | Small Corpus               |\n",
    "| Example       | Predict “sat” from context | Predict context from “sat” |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Intuitive Understanding of Context\n",
    "\n",
    "Context in Word2Vec refers to the **surrounding words** of a target word within a fixed-size **window**.\n",
    "\n",
    "Example sentence:\n",
    "\n",
    "> “The dog barked loudly at night.”\n",
    "\n",
    "With **window size = 2**, for the target word **“barked”**, the context words are:\n",
    "\n",
    "```\n",
    "[\"The\", \"dog\", \"loudly\", \"at\"]\n",
    "```\n",
    "\n",
    "These contexts help Word2Vec learn that:\n",
    "\n",
    "* “barked” often appears with “dog”.\n",
    "* So, “barked” and “dog” should be **semantically close** in vector space.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 Conclusion\n",
    "\n",
    "Word2Vec revolutionized NLP by introducing **dense, meaningful word representations** that preserve **semantic relationships**.\n",
    "\n",
    "It laid the foundation for advanced **contextual embeddings** such as:\n",
    "\n",
    "* **GloVe (Global Vectors)**\n",
    "* **FastText**\n",
    "* **BERT**, **GPT**, and other Transformer-based models\n",
    "\n",
    "> 🗣️ In short: *Word2Vec taught machines to understand the meaning of words through context.*\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 References\n",
    "\n",
    "* Mikolov et al., “Efficient Estimation of Word Representations in Vector Space”, Google Research (2013)\n",
    "* [Gensim Word2Vec Documentation](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "* [TensorFlow Word2Vec Tutorial](https://www.tensorflow.org/tutorials/text/word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1113b43-d69e-47d3-9dfe-52aa985513d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fd8147a-d6a3-4e31-bf90-1f312b1d60d0",
   "metadata": {},
   "source": [
    "# 🧩 Understanding Word Embedding Vectors — Google Word2Vec Example\n",
    "\n",
    "When Google introduced **Word2Vec**, one of the most powerful discoveries was that the learned **word vectors** captured real-world **semantic relationships**.\n",
    "\n",
    "For example:\n",
    "> **vector(\"King\") - vector(\"Man\") + vector(\"Woman\") ≈ vector(\"Queen\")**\n",
    "\n",
    "This means the model **learned the concept of gender and royalty** — purely from reading text, without being explicitly told about these relationships!\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 Step-by-Step Concept\n",
    "\n",
    "1️⃣ **Vocabulary (Unique Words)**  \n",
    "   - These are all the unique words in your corpus (training text).\n",
    "\n",
    "   Example vocabulary:\n",
    "```\n",
    "\n",
    "[Boy, Girl, King, Queen, Apple, Mango]\n",
    "\n",
    "```\n",
    "\n",
    "2️⃣ **Each word** is represented as a **dense vector** — not just 0s and 1s.  \n",
    "- Each dimension captures a certain **hidden feature** (e.g., Gender, Royalty, Age, Food, etc.).\n",
    "\n",
    "3️⃣ These dimensions are **not predefined** — the model **learns** them automatically during training.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Dummy Example: Word2Vec Feature Space\n",
    "\n",
    "Below is a **simplified (dummy)** representation of how Word2Vec might internally encode meaning:\n",
    "\n",
    "| Word  | Gender | Royalty | Age | Food | Sweetness | Objectness |\n",
    "|--------|:-------:|:-------:|:----:|:----:|:----------:|:------------:|\n",
    "| **Boy**   | -0.95 |  0.05 |  0.70 |  0.00 |  0.00 |  0.10 |\n",
    "| **Girl**  |  0.97 |  0.04 |  0.68 |  0.00 |  0.00 |  0.12 |\n",
    "| **King**  | -0.90 |  0.95 |  0.80 |  0.00 |  0.00 |  0.15 |\n",
    "| **Queen** |  0.92 |  0.96 |  0.78 |  0.00 |  0.00 |  0.14 |\n",
    "| **Apple** |  0.00 |  0.00 |  0.00 |  0.98 |  0.95 |  0.05 |\n",
    "| **Mango** |  0.00 |  0.00 |  0.00 |  0.97 |  0.96 |  0.06 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Interpretation\n",
    "\n",
    "- **Gender Dimension:**  \n",
    "Positive values → feminine (e.g., *Girl*, *Queen*)  \n",
    "Negative values → masculine (e.g., *Boy*, *King*)\n",
    "\n",
    "- **Royalty Dimension:**  \n",
    "High for *King* and *Queen*, low for *Boy*, *Girl*, *Apple*, etc.\n",
    "\n",
    "- **Food Dimension:**  \n",
    "High for *Apple* and *Mango*, nearly zero for human-related words.\n",
    "\n",
    "- **Sweetness Dimension:**  \n",
    "High for fruits, zero for non-food items.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 Semantic Relationships Captured\n",
    "\n",
    "| Relationship | Vector Math | Meaning |\n",
    "|---------------|--------------|----------|\n",
    "| Gender | **King - Man + Woman ≈ Queen** | The model understands gender analogy. |\n",
    "| Age | **Boy - Young + Adult ≈ Man** | Learns age-based transformation. |\n",
    "| Category | **Apple - Fruit + Country ≈ Japan** | Learns contextual category shift. |\n",
    "| Synonyms | **Big ≈ Large**, **Fast ≈ Quick** | Close vectors for similar words. |\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Visual Analogy (2D Projection Example)\n",
    "\n",
    "Word2Vec embeddings exist in **hundreds of dimensions**, but when visualized in **2D** (via PCA or t-SNE), you can see meaningful clusters:\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "       Royalty ↑\n",
    "            │                Queen\n",
    "            │             King\n",
    "            │        Prince\n",
    "            │\n",
    "            │\n",
    "```\n",
    "\n",
    "Gender  ────────┼────────────────────────→\n",
    "│\n",
    "│          Girl\n",
    "│     Boy\n",
    "│\n",
    "│\n",
    "\n",
    "```\n",
    "\n",
    "- Words like *Boy–Girl* and *King–Queen* align in similar directions along the **Gender axis**.\n",
    "- Words like *King–Queen–Prince* cluster along the **Royalty axis**.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Key Takeaway\n",
    "\n",
    "Word2Vec doesn’t just memorize words —  \n",
    "it **understands** them through **contextual relationships**.\n",
    "\n",
    "The model learns:\n",
    "- Similar words appear in similar contexts.\n",
    "- Context defines meaning.\n",
    "- Arithmetic operations on vectors reveal hidden semantic patterns.\n",
    "\n",
    "> 🗣️ Example:  \n",
    "> `King - Man + Woman ≈ Queen`  \n",
    "> demonstrates that **Word2Vec captures real-world logic** in numerical form.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 Real-World Embedding Space Snapshot (from Google’s Paper)\n",
    "\n",
    "Google’s trained model on **Google News dataset (100B words)** produced embeddings that naturally aligned like this:\n",
    "\n",
    "| Relationship | Example Pairs | Observed Result |\n",
    "|---------------|---------------|-----------------|\n",
    "| Gender | (man → woman), (king → queen) | Consistent vector direction |\n",
    "| Verb tense | (walk → walked), (eat → ate) | Captures grammatical rules |\n",
    "| Country–Capital | (France → Paris), (Italy → Rome) | Geographic understanding |\n",
    "| Comparative | (big → bigger), (fast → faster) | Learns adjective relationships |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Summary\n",
    "\n",
    "| Concept | Description |\n",
    "|----------|--------------|\n",
    "| **Vocabulary** | Set of all unique words in the corpus. |\n",
    "| **Corpus** | The entire collection of text used for training. |\n",
    "| **Embedding Vector** | Numeric representation of a word in multi-dimensional space. |\n",
    "| **Context** | Surrounding words that define meaning. |\n",
    "| **Semantic Space** | The multi-dimensional field where similar words lie close together. |\n",
    "\n",
    "---\n",
    "\n",
    "✨ **In essence:**\n",
    "> Word2Vec transforms raw words into a structured semantic world — where relationships, categories, and analogies are all embedded in the geometry of vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a73f2f3-caa1-4c43-93dc-17a2ef359068",
   "metadata": {},
   "source": [
    "# 📏 Cosine Similarity in Word Embedding\n",
    "\n",
    "## 📖 What is Cosine Similarity?\n",
    "\n",
    "When we represent words as **vectors**, we can measure how **similar** two words are by checking the **angle** between their vectors — not their absolute distance.\n",
    "\n",
    "That’s where **cosine similarity** comes in.\n",
    "\n",
    "It measures the **cosine of the angle** between two vectors in the embedding space.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Formula\n",
    "\n",
    "For two vectors **A** and **B**, the **cosine similarity** is defined as:\n",
    "\n",
    "\\[\n",
    "\\text{Cosine Similarity (A, B)} = \\frac{A \\cdot B}{||A|| \\times ||B||}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( A \\cdot B \\) = dot product of vectors A and B  \n",
    "- \\( ||A|| \\) and \\( ||B|| \\) = magnitude (length) of A and B\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Intuition\n",
    "\n",
    "- Cosine similarity only depends on the **angle between vectors**, not their length.\n",
    "- Two words are **similar** if their vectors point in the **same direction**.\n",
    "\n",
    "### Example:\n",
    "\n",
    "| Pair | Cosine Similarity | Meaning |\n",
    "|-------|-------------------|----------|\n",
    "| King – Queen | 0.92 | Very similar (same gender and royalty context) |\n",
    "| King – Man | 0.75 | Moderately similar (gender relation) |\n",
    "| King – Apple | 0.12 | Not similar (different semantic fields) |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Visual Representation\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "   ↑ Royalty\n",
    "   │\n",
    "   │       Queen\n",
    "   │      /\n",
    "   │     /\n",
    "   │    /\n",
    "   │   /   (small angle → high similarity)\n",
    "   │  /\n",
    "   │ /      \n",
    "   │/________________→ Gender\n",
    "  King\n",
    "```\n",
    "\n",
    "````\n",
    "\n",
    "In this diagram:\n",
    "- The **angle** between \"King\" and \"Queen\" is **small**, so **cosine similarity ≈ 1**.\n",
    "- If the vectors were perpendicular (90°), similarity ≈ 0 (no relation).\n",
    "- If opposite (180°), similarity ≈ -1 (opposite meanings).\n",
    "\n",
    "---\n",
    "\n",
    "## ⚖️ Cosine Similarity vs Euclidean Distance\n",
    "\n",
    "| Aspect | Cosine Similarity | Euclidean Distance |\n",
    "|---------|-------------------|--------------------|\n",
    "| **Definition** | Measures **angle** between two vectors | Measures **straight-line distance** between points |\n",
    "| **Range** | -1 to 1 | 0 to ∞ |\n",
    "| **Focus** | Orientation (direction) | Magnitude (distance) |\n",
    "| **When Useful** | When vector length doesn’t matter (e.g., text) | When absolute distance matters (e.g., coordinates) |\n",
    "| **In Word2Vec** | ✅ Preferred (semantic comparison) | ⚠️ Not suitable (scale varies) |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Why Word2Vec Uses Cosine Similarity\n",
    "\n",
    "- In Word2Vec, vector magnitudes are **not important** — what matters is the **relative direction**.\n",
    "- Two words with **similar meanings** (e.g., “strong” and “powerful”) appear in **similar contexts**, resulting in **vectors pointing in similar directions**.\n",
    "- Thus, cosine similarity effectively measures **semantic closeness**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Example Calculation\n",
    "\n",
    "Let:\n",
    "\\[\n",
    "A = [1, 2, 3], \\quad B = [2, 3, 4]\n",
    "\\]\n",
    "\n",
    "Then:\n",
    "\\[\n",
    "A \\cdot B = (1)(2) + (2)(3) + (3)(4) = 20\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "||A|| = \\sqrt{1^2 + 2^2 + 3^2} = 3.74, \\quad ||B|| = \\sqrt{2^2 + 3^2 + 4^2} = 5.38\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\text{Cosine Similarity} = \\frac{20}{(3.74 \\times 5.38)} ≈ 0.995\n",
    "\\]\n",
    "\n",
    "✅ Result: Vectors are almost in the same direction → **Highly similar words**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 Example in Word2Vec (Python - Gensim)\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [\n",
    "    [\"king\", \"queen\", \"man\", \"woman\", \"apple\", \"mango\"]\n",
    "]\n",
    "\n",
    "# Train model\n",
    "model = Word2Vec(sentences, vector_size=10, min_count=1, sg=1)\n",
    "\n",
    "# Cosine similarity between words\n",
    "print(model.wv.similarity('king', 'queen'))\n",
    "print(model.wv.similarity('king', 'apple'))\n",
    "````\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "king vs queen: 0.89\n",
    "king vs apple: 0.10\n",
    "```\n",
    "\n",
    "👉 Words closer in **meaning** → higher cosine similarity.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Summary: Vector Distance Metrics\n",
    "\n",
    "| Metric                 | Measures                    | Range  | Word2Vec Use | Comment                                |\n",
    "| ---------------------- | --------------------------- | ------ | ------------ | -------------------------------------- |\n",
    "| **Cosine Similarity**  | Angle (direction)           | -1 → 1 | ✅ Yes        | Best for comparing semantic similarity |\n",
    "| **Euclidean Distance** | Magnitude difference        | 0 → ∞  | ⚠️ No        | Sensitive to vector scale              |\n",
    "| **Manhattan Distance** | Sum of absolute differences | 0 → ∞  | ❌ Rare       | Not rotation invariant                 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 Real-Life Analogy\n",
    "\n",
    "Imagine each word as a **direction** in meaning space.\n",
    "\n",
    "* “King” and “Queen” point toward **royalty**.\n",
    "* “Apple” and “Mango” point toward **fruits**.\n",
    "* “King” and “Apple” point in **very different directions**, so the angle is large → low similarity.\n",
    "\n",
    "```\n",
    "               🍎 Mango\n",
    "               /\n",
    "              /\n",
    "      👑 Queen\n",
    "             \\\n",
    "              \\\n",
    "               👑 King\n",
    "               \\\n",
    "                \\\n",
    "                 🍏 Apple\n",
    "```\n",
    "\n",
    "Even though all vectors have roughly the same length (magnitude), **direction** defines meaning — that’s why **cosine similarity** is ideal for **text embeddings**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Key Takeaways\n",
    "\n",
    "| Concept                          | Description                                                  |\n",
    "| -------------------------------- | ------------------------------------------------------------ |\n",
    "| **Cosine Similarity**            | Measures the angle between two word vectors.                 |\n",
    "| **Higher Value (close to 1)**    | Words have similar meanings and contexts.                    |\n",
    "| **Lower Value (close to 0)**     | Words are unrelated.                                         |\n",
    "| **Negative Value (close to -1)** | Words have opposite meanings.                                |\n",
    "| **Used In**                      | Word2Vec, Doc2Vec, BERT, GPT, and almost all NLP embeddings. |\n",
    "\n",
    "---\n",
    "\n",
    "> 🗣️ **In summary:**\n",
    "> Cosine similarity gives Word2Vec the ability to **compare meanings geometrically**, not just by frequency — turning language into measurable mathematics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae89d1-7e22-4cec-b776-3f01810abc4f",
   "metadata": {},
   "source": [
    "# CBOW (Continuous Bag of Words) — Neural Network Diagram & Full Details\n",
    "\n",
    "> Corpus example used in this document:\n",
    ">\n",
    "> `iNeruon company is related to data science`\n",
    ">\n",
    "> Window size = 5 (context size). Target word predicted from context words (CBOW).\n",
    "\n",
    "---\n",
    "\n",
    "## 1 — Quick overview\n",
    "\n",
    "CBOW predicts the target word given surrounding context words. Each context word is converted to a one-hot vector (size = vocabulary size V), mapped to an embedding space (embedding size = D). The embeddings for the context words are averaged (or summed) and fed into a fully connected layer followed by a softmax over the vocabulary to produce a probability distribution for the target word.\n",
    "\n",
    "Key components in this doc:\n",
    "\n",
    "* Vocabulary and one-hot encoding example from your corpus\n",
    "* Detailed architecture diagram (ASCII + Mermaid)\n",
    "* Shapes of weight matrices and forward/backward pass math\n",
    "* Loss, optimization, and training details\n",
    "* Implementation sketch (Keras / PyTorch)\n",
    "* Variants (negative sampling, skip-gram), tips and hyperparameters\n",
    "\n",
    "---\n",
    "\n",
    "## 2 — Vocabulary & One-Hot encoding (example)\n",
    "\n",
    "Assume after tokenization/normalization our vocabulary (V = 7) is:\n",
    "\n",
    "```\n",
    "0: ineruon\n",
    "1: company\n",
    "2: is\n",
    "3: related\n",
    "4: to\n",
    "5: data\n",
    "6: science\n",
    "```\n",
    "\n",
    "One-hot vectors (length V = 7) for the first four vocabulary words:\n",
    "\n",
    "```\n",
    "ineruon   -> [1 0 0 0 0 0 0]\n",
    "company   -> [0 1 0 0 0 0 0]\n",
    "is        -> [0 0 1 0 0 0 0]\n",
    "related   -> [0 0 0 1 0 0 0]\n",
    "... and so on\n",
    "```\n",
    "\n",
    "> Note: Real datasets typically have much larger V (thousands to millions). For educational diagrams we use V = 7.\n",
    "\n",
    "---\n",
    "\n",
    "## 3 — CBOW architecture (high-level)\n",
    "\n",
    "Mermaid diagram (can be rendered by a renderer that supports Mermaid):\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  subgraph Input [Input (context words)]\n",
    "    C1[one-hot w_{t-2}]\n",
    "    C2[one-hot w_{t-1}]\n",
    "    C3[one-hot w_{t+1}]\n",
    "    C4[one-hot w_{t+2}]\n",
    "  end\n",
    "\n",
    "  C1 -->|multiply by| E1[Embedding lookup: E (V x D)]\n",
    "  C2 -->|multiply by| E2[Embedding lookup: E]\n",
    "  C3 -->|multiply by| E3[Embedding lookup: E]\n",
    "  C4 -->|multiply by| E4[Embedding lookup: E]\n",
    "\n",
    "  E1 --> Avg[Average / Sum]\n",
    "  E2 --> Avg\n",
    "  E3 --> Avg\n",
    "  E4 --> Avg\n",
    "\n",
    "  Avg --> FC{Fully connected: W' (D x V) + b'}\n",
    "  FC --> Softmax[Softmax over V]\n",
    "  Softmax --> Output[(Predicted target word distribution)]\n",
    "```\n",
    "\n",
    "### ASCII-style neural net diagram (small example)\n",
    "\n",
    "```\n",
    " Context one-hot vectors (4 context words, V=7)         Hidden/embedding layer (D=50 shown abstractly)\n",
    " [1 0 0 0 0 0 0]--> [embedding vector e1]  \\            [avg e] ---> [W'.T] --> softmax (size V)\n",
    " [0 1 0 0 0 0 0]--> [embedding vector e2]  /           \n",
    " [0 0 0 1 0 0 0]--> [embedding vector e3]\n",
    " [0 0 0 0 1 0 0]--> [embedding vector e4]\n",
    "\n",
    " Final predicted distribution over vocabulary -> target: 'is'\n",
    "```\n",
    "---\n",
    "\n",
    "## 4 — Shapes and parameters (concrete)\n",
    "\n",
    "Let:\n",
    "\n",
    "* V = vocabulary size (7 in our toy example)\n",
    "* D = embedding size (we pick D = 50 for demonstration; you can choose 50, 100, 200, etc.)\n",
    "* C = number of context words (window size minus target; for window size 5 centered, C = 4 context words)\n",
    "\n",
    "**Weight matrices**:\n",
    "\n",
    "* Embedding matrix `W_e` (also called `E`): shape `(V, D)` — maps one-hot (V) to embedding (D). In implementation it's typically a lookup table, not computed as full matrix multiplication for efficiency.\n",
    "* Output weight matrix `W_out` (sometimes `W'`): shape `(D, V)` — maps averaged embedding to un-normalized scores for each vocabulary word.\n",
    "* Bias `b_out`: shape `(V,)`.\n",
    "\n",
    "**Forward pass math** (for one training sample):\n",
    "\n",
    "1. Convert each context word `w_i` to one-hot `x_i` (V-dim). Embedding lookup: `e_i = W_e^T x_i` (shape D).\n",
    "2. Average embeddings: `e = (1/C) * sum_{i=1..C} e_i` (shape D).\n",
    "3. Score vector: `u = W_out^T e + b_out` (shape V).\n",
    "4. Probability by softmax: `y_hat = softmax(u)`.\n",
    "5. Loss for target word index t: `L = -log(y_hat[t])`.\n",
    "\n",
    "**Parameter counts**:\n",
    "\n",
    "* Embeddings: `V * D` parameters.\n",
    "* Output: `D * V + V` (weights + bias).\n",
    "* Total ~ `2*V*D + V`.\n",
    "\n",
    "For toy numbers: V=7, D=50 -> embeddings = 350 params, output weights = 350 params, bias = 7 -> total ~ 707 parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## 5 — Example forward pass using your sample\n",
    "\n",
    "Corpus window example (window size = 5): target is center word. Example line from your prompt:\n",
    "\n",
    "Inputs (context) -> Target\n",
    "\n",
    "* `iNeruon, company, related, to` -> `is`\n",
    "* `Company, Is, To, Data` -> `Related`\n",
    "* `is, related, data, science` -> `to`\n",
    "\n",
    "Take first example: context words indices [0,1,3,4] -> embeddings e0,e1,e3,e4 -> average `e` -> compute `u=W_out^T e + b` -> softmax -> highest probability ideally for index of `is`.\n",
    "\n",
    "---\n",
    "\n",
    "## 6 — Training details\n",
    "\n",
    "**Loss**: Cross-entropy negative log-likelihood: `L = -log(y_hat[target])`.\n",
    "\n",
    "**Optimization**: SGD, Adam, RMSprop. Typical learning rates:\n",
    "\n",
    "* SGD: 0.01–0.5 (with careful scheduling)\n",
    "* Adam: 1e-3 (good default)\n",
    "\n",
    "**Batching**: Use minibatches (e.g., 256–2048 samples per batch depending on memory).\n",
    "\n",
    "**Regularization**: dropout is rarely used on embeddings; L2 on output weights sometimes used.\n",
    "\n",
    "**Speedups for large V**: softmax over very large vocab is expensive. Alternatives:\n",
    "\n",
    "* Negative sampling (skip-gram negative sampling adapted to CBOW) — sample a few negative words and train with logistic loss.\n",
    "* Hierarchical softmax — tree-based decomposition of softmax.\n",
    "\n",
    "**Epochs**: Several passes over the corpus — often large corpora need just a few epochs; toy corpora may overfit quickly.\n",
    "\n",
    "---\n",
    "\n",
    "## 7 — Variants & notes\n",
    "\n",
    "* **CBOW vs Skip-gram**: CBOW predicts word from context; skip-gram predicts context from a word. Skip-gram with negative sampling (SGNS) is popular (Word2Vec original paper).\n",
    "* **Averaging vs Summation**: You can sum embeddings or average. Averaging normalizes by context size.\n",
    "* **Position weighting**: Optionally weight embeddings based on distance from center word.\n",
    "* **Sub-sampling frequent words**: For large datasets, subsample very frequent words (e.g., `the`, `is`) to speed up training and improve embedding quality.\n",
    "\n",
    "---\n",
    "\n",
    "## 8 — Implementation sketch (Keras)\n",
    "\n",
    "```python\n",
    "# Simple CBOW-like model in Keras (toy, illustrative)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Lambda, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "V = 10000  # vocab size (toy)\n",
    "D = 50     # embedding size\n",
    "C = 4      # number of context words\n",
    "\n",
    "# Inputs: C integers (context word indices)\n",
    "inputs = [Input(shape=(1,), dtype='int32') for _ in range(C)]\n",
    "\n",
    "emb = Embedding(input_dim=V, output_dim=D, input_length=1)  # W_e\n",
    "embeddings = [emb(inp) for inp in inputs]  # list of shape (None,1,D)\n",
    "\n",
    "# remove the length-1 dimension and average\n",
    "embeddings = [Lambda(lambda x: K.squeeze(x, axis=1))(e) for e in embeddings]\n",
    "avg = Lambda(lambda x: K.mean(K.stack(x, axis=1), axis=1))(emb\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a92002-4583-4c96-975c-b060bf18ac29",
   "metadata": {},
   "source": [
    "![CBOW Neural Network](assets/Word2Vec-CBOW-1.png)\n",
    "![CBOW Neural Network](assets/Word2Vec-CBOW-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d2f4a0-ca92-40eb-9a70-d5037cde7f0c",
   "metadata": {},
   "source": [
    "## SkipGram\n",
    "Corpus:\n",
    "    iNeruon company is releted to data science.\n",
    "    window size = 5\n",
    "\n",
    "I/P                                        O/P\n",
    "-> iNeruon, company, Related to             is\n",
    "-> Company, Is, To, Data                    Related\n",
    "-> is, related, data, science               to\n",
    "\n",
    "One Hot encoding\n",
    "iNeruon  1 0 0 0 0 0 0 \n",
    "company  0 1 0 0 0 0 0\n",
    "related  0 0 0 1 0 0 0\n",
    "to       0 0 0 0 1 0 0\n",
    "\n",
    "   weights\n",
    "I/P    W.S.           O/P\n",
    "0      0         0 0 0 0 0 0 0   5*7\n",
    "0      0         0 0 0 0 0 0 0   5*7     \n",
    "0      0         0 0 0 0 0 0 0   5*7\n",
    "0      0         0 0 0 0 0 0 0   5*7\n",
    "0      0         0 0 0 0 0 0 0   5*7\n",
    "0      7*5\n",
    "0\n",
    "\n",
    "\n",
    "When should be apply CBOW or skipgram\n",
    "- Small dataset(corpus) ->> CBOW\n",
    "- Huge Dataset(corpus)  ->> SkipGram\n",
    "\n",
    "\n",
    "How to improve\n",
    "CBOW or Skipgram\n",
    "1) Increase the training data\n",
    "2) increase the window size which in leads to increase of vector dimension\n",
    "\n",
    "Google WOrd2Vec\n",
    "- 3 billion words -> google news\n",
    "- feature representation of 300 dimention vectors\n",
    "  cricket [............300] dimention vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159974cb-d391-4f06-9aab-bd9a10e5f9fa",
   "metadata": {},
   "source": [
    "# Word2Vec — CBOW and Skip-Gram (Complete Guide)\n",
    "\n",
    "> A compact, runnable Markdown guide covering intuition, math, training, practical tips, and a fully-connected ANN diagram to understand CBOW and Skip-Gram. Includes a small worked example (your corpus) and useful code snippets for Jupyter.\n",
    "\n",
    "--- \n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. Overview: what is Word2Vec?\n",
    "2. One-hot encoding and the input/output setup\n",
    "3. CBOW — intuition, math, forward/backward pass, training objective\n",
    "4. Skip-Gram — intuition, math, forward/backward pass, training objective\n",
    "5. Softmax, hierarchical softmax, negative sampling (why & when)\n",
    "6. Worked example (your corpus) — step-by-step calculation\n",
    "7. Fully connected ANN diagram (ASCII + HTML-friendly SVG snippet)\n",
    "8. When to use CBOW vs Skip-Gram\n",
    "9. How to improve results (hyperparams, data, tricks)\n",
    "10. Google Word2Vec facts\n",
    "11. Quick code examples (gensim + pure NumPy toy model)\n",
    "12. Practical tips for Jupyter and embedding images in Markdown\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview: what is Word2Vec?\n",
    "\n",
    "Word2Vec is a family of two-layer neural-network models that learn dense vector representations (embeddings) for words such that words with similar contexts have similar vectors. The two primary architectures are:\n",
    "\n",
    "* **CBOW (Continuous Bag of Words):** predict the center word from surrounding context words.\n",
    "* **Skip-Gram:** predict surrounding context words given the center word.\n",
    "\n",
    "Both map one-hot encoded words (very high-dimensional, sparse) into a low-dimensional dense vector space (e.g., 100–300 dims).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. One-hot encoding and I/O setup\n",
    "\n",
    "Given a vocabulary of size `V`, each word is a one-hot vector of length `V` (all zeros except a single 1 at the word index).\n",
    "\n",
    "Example small vocab and one-hot matrix (vertical = indices):\n",
    "\n",
    "```\n",
    "Vocab: [iNeruon, company, related, to, data, science, is]\n",
    "One-hot:\n",
    " iNeruon  [1 0 0 0 0 0 0]\n",
    " company  [0 1 0 0 0 0 0]\n",
    " related  [0 0 1 0 0 0 0]\n",
    " to       [0 0 0 1 0 0 0]\n",
    " data     [0 0 0 0 1 0 0]\n",
    " science  [0 0 0 0 0 1 0]\n",
    " is       [0 0 0 0 0 0 1]\n",
    "```\n",
    "\n",
    "**Weight matrices:**\n",
    "\n",
    "* `W1` (V × N): input-to-hidden weight matrix (rows correspond to input words, columns to embedding dims).\n",
    "* `W2` (N × V): hidden-to-output weight matrix.\n",
    "\n",
    "For CBOW, multiple input one-hot vectors are averaged (or summed) to produce the hidden activation. For Skip-Gram, a single input one-hot flows through to hidden.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. CBOW — intuition & math\n",
    "\n",
    "**Intuition:** Given the context words (surrounding words inside a window), predict the center word. Effective for smaller datasets and faster to train.\n",
    "\n",
    "**Forward pass (single training example):**\n",
    "\n",
    "1. Convert each context word into its one-hot vector and multiply by `W1` to obtain embeddings, or equivalently select the corresponding rows of `W1`.\n",
    "2. Compute the average (or sum) of those embeddings: `h = (1 / C) * sum_{i=1..C} v_{context_i}`.\n",
    "3. Compute scores for each vocabulary word: `u = W2^T h` (shape V).\n",
    "4. Apply softmax over `u` to get probabilities `y_hat`.\n",
    "\n",
    "**Loss:** negative log-likelihood (cross-entropy):\n",
    "\n",
    "```\n",
    "L = -log( softmax(u_center) )\n",
    "```\n",
    "\n",
    "**Backprop:** compute gradient wrt `W2` and `W1` (only rows corresponding to context words get updated). When implementing, you can use efficient matrix operations to update multiple rows.\n",
    "\n",
    "**Batching:** CBOW naturally supports batching because you average several inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Skip-Gram — intuition & math\n",
    "\n",
    "**Intuition:** Given the target (center) word, predict each context word (many predictions per center). Works very well with large corpora and captures rare-word representations better.\n",
    "\n",
    "**Forward pass (one center word, multiple output predictions):**\n",
    "\n",
    "1. Center word one-hot → select row of `W1` → `h` (embedding of center).\n",
    "2. For each context position, compute `u = W2^T h` → softmax → probability for that context word.\n",
    "3. Loss is sum of cross-entropies over context words.\n",
    "\n",
    "**Training:** Because Skip-Gram predicts many outputs per input, it performs better on large corpora. However, naive softmax over full vocab is expensive — that's where negative sampling and hierarchical softmax come in.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Softmax, hierarchical softmax, negative sampling\n",
    "\n",
    "**Full softmax:** computes probabilities over entire vocabulary `V`. Complexity = `O(V)` per prediction — expensive for big vocabularies.\n",
    "\n",
    "**Hierarchical softmax:** represents words as leaves of a binary tree; computing word probability takes `O(log V)`.\n",
    "\n",
    "**Negative sampling (very common):** for each positive center-context pair, sample `k` negative words from a noise distribution and instead of softmax use logistic loss for positive and negative pairs. Complexity becomes `O(k)` per pair (k often between 5 and 20).\n",
    "\n",
    "Loss for negative sampling for a positive pair (center `w_c`, context `w_o`):\n",
    "\n",
    "```\n",
    "log σ(v_{w_o}^T v_{w_c}) + sum_{i=1..k} E_{w_i~P_n(w)}[ log σ(-v_{w_i}^T v_{w_c}) ]\n",
    "```\n",
    "\n",
    "where `σ` is sigmoid and `P_n(w)` is noise distribution (often unigram^0.75).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Worked example (your corpus)\n",
    "\n",
    "Corpus sentence: `iNeruon company is releted to data science.` (I'll correct spelling `releted -> related`)\n",
    "Window size = 2 (for demo; you wrote 5 but we'll use 2 to keep steps short). Vocabulary: `[iNeruon, company, related, to, data, science, is]` (V=7)\n",
    "\n",
    "### Example Skip-Gram pair generation (window=2):\n",
    "\n",
    "Take center `related` (index 3); context words within window 2 are `iNeruon, company, is, to` depending on sentence boundaries.\n",
    "So training pairs: `(related -> iNeruon)`, `(related -> company)`, `(related -> is)`, `(related -> to)`\n",
    "\n",
    "One-hot for `related` = `[0,0,1,0,0,0,0]`.\n",
    "\n",
    "If embedding dim `N = 3` (toy):\n",
    "`W1` (7×3) and `W2` (3×7) initialized randomly. Forward: `h = W1[row_index_of_related]` (shape 3). Compute `u = W2^T h` (shape 7). Apply softmax, compute loss for target e.g., `iNeruon`.\n",
    "\n",
    "I won't run numbers here (they are straightforward matrix multiplies). If you want numerical steps, I can show a single update with random small matrices.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Fully-connected ANN diagram (ASCII + embeddable SVG)\n",
    "\n",
    "Below is a simple fully-connected ANN representation for the CBOW/Skip-Gram one-hidden-layer architecture used by Word2Vec.\n",
    "\n",
    "**ASCII diagram (CBOW, averaging 4 context words → hidden → output):**\n",
    "\n",
    "```\n",
    "Context one-hot vectors (V)   Context one-hot vectors (V)   ...   Context one-hot vectors (V)\n",
    "      |                             |                              | \n",
    "      v                             v                              v\n",
    "   [select row]                 [select row]                   [select row]\n",
    "      \\           (sum/avg)   /      \n",
    "        \\        /-----\\     /       \n",
    "          \\    /  W1   \\  /         \n",
    "            -->| (VxN) |-->  h (N-d embedding)\n",
    "                 \\-----/\n",
    "                    |\n",
    "                    v\n",
    "                  W2^T (N x V)  => raw scores (V)\n",
    "                    |\n",
    "                    v\n",
    "                  softmax\n",
    "                    |\n",
    "                    v\n",
    "                 output probs (V)\n",
    "```\n",
    "\n",
    "**Interpretation:** The hidden layer is the embedding. For CBOW we aggregate context embeddings; for Skip-Gram the input is a single one-hot that selects a single embedding which is used to predict multiple outputs.\n",
    "\n",
    "**HTML/SVG snippet (paste into an HTML cell in Jupyter or an `.html` file):**\n",
    "\n",
    "```html\n",
    "<!-- Small SVG to visualize the single-hidden-layer Word2Vec network -->\n",
    "<svg width=\"700\" height=\"260\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "  <rect x=\"10\" y=\"30\" width=\"120\" height=\"200\" fill=\"#f3f4f6\" stroke=\"#ccc\" />\n",
    "  <text x=\"22\" y=\"50\">Input (one-hot)</text>\n",
    "  <r\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ea8576-1d23-4e37-8829-4388031e7409",
   "metadata": {},
   "source": [
    "## 🟩 Advantages of Word2Vec\n",
    "\n",
    "1. **Sparse Matrix → Dense Matrix**\n",
    "\n",
    "   * Traditional models like Bag of Words or TF-IDF produce large **sparse matrices** (mostly zeros).\n",
    "   * Word2Vec learns **dense, low-dimensional representations** (e.g., 100–300 dimensions), which are compact and computationally efficient.\n",
    "\n",
    "2. **Captures Semantic Relationships**\n",
    "\n",
    "   * Words with similar meanings or used in similar contexts have **similar vector representations**.\n",
    "   * Example:\n",
    "\n",
    "     ```\n",
    "     vec(\"king\") - vec(\"man\") + vec(\"woman\") ≈ vec(\"queen\")\n",
    "     ```\n",
    "   * Captures semantic similarity (e.g., *good*, *honest*, *kind*) and syntactic relations (e.g., *walking*, *walked*, *walks*).\n",
    "\n",
    "3. **Fixed Vector Dimension for All Words**\n",
    "\n",
    "   * Every word in the vocabulary is represented by a **vector of fixed dimension** (e.g., 300-D in Google Word2Vec).\n",
    "   * This consistency simplifies model input for downstream ML tasks.\n",
    "\n",
    "4. **Handles Large Vocabulary Efficiently**\n",
    "\n",
    "   * Word2Vec uses **negative sampling** and **hierarchical softmax** to train on millions of words efficiently.\n",
    "\n",
    "5. **Improves NLP Performance**\n",
    "\n",
    "   * Pretrained embeddings can significantly improve the accuracy of NLP models for classification, translation, and sentiment analysis tasks.\n",
    "\n",
    "6. **Generalization**\n",
    "\n",
    "   * Embeddings generalize well across tasks; pretrained models like Google’s Word2Vec can be reused for many NLP problems.\n",
    "\n",
    "---\n",
    "\n",
    "## 🟥 Disadvantages of Word2Vec\n",
    "\n",
    "1. **OOV (Out-of-Vocabulary) Problem Not Truly Solved**\n",
    "\n",
    "   * If a word wasn’t seen during training, Word2Vec **can’t produce an embedding** for it.\n",
    "   * It doesn’t handle new or rare words gracefully (later models like **FastText** fixed this using subword embeddings).\n",
    "\n",
    "2. **Context-Independent Representations**\n",
    "\n",
    "   * Each word has **only one vector**, regardless of context.\n",
    "\n",
    "     * Example: The word *“bank”* means different things in *“river bank”* and *“bank account”*, but Word2Vec gives it a single vector.\n",
    "   * Contextual models like **BERT** or **ELMo** overcome this.\n",
    "\n",
    "3. **Requires Large Training Data**\n",
    "\n",
    "   * To learn meaningful embeddings, it needs **huge text corpora** (billions of words).\n",
    "   * Poor results on small datasets.\n",
    "\n",
    "4. **Cannot Handle Out-of-Domain Words**\n",
    "\n",
    "   * If trained on a specific domain (e.g., news), it may not perform well on another (e.g., medical or legal text).\n",
    "\n",
    "5. **Black-box Nature**\n",
    "\n",
    "   * The embeddings are not directly interpretable — you can’t easily tell why two words are close in vector space without analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary Table\n",
    "\n",
    "| Aspect                        | Word2Vec                |\n",
    "| ----------------------------- | ----------------------- |\n",
    "| **Matrix Type**               | Dense (efficient)       |\n",
    "| **Semantic Meaning**          | Captured                |\n",
    "| **Vector Dimension**          | Fixed (e.g., 300)       |\n",
    "| **OOV Words**                 | ❌ Not handled           |\n",
    "| **Context Awareness**         | ❌ Not context-dependent |\n",
    "| **Training Data Requirement** | Large                   |\n",
    "| **Interpretability**          | Limited                 |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1cd81-024b-4348-9b63-00c503a96ea6",
   "metadata": {},
   "source": [
    "# Word2Vec — CBOW, Skip-Gram, and Average Word2Vec (Complete Guide)\n",
    "\n",
    "> This document explains Word2Vec architectures (CBOW and Skip-Gram), their advantages/disadvantages, and how **Average Word2Vec** is used for text classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Recap: What is Word2Vec?\n",
    "\n",
    "Word2Vec transforms words into **dense vector representations** (embeddings) that capture semantic and syntactic meaning. Each word is represented by a fixed-size vector (e.g., 100–300 dimensions).\n",
    "\n",
    "Two training approaches:\n",
    "\n",
    "* **CBOW (Continuous Bag of Words):** Predict target word from context.\n",
    "* **Skip-Gram:** Predict context words from the target word.\n",
    "\n",
    "Both learn embeddings via a shallow neural network (one hidden layer).\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Average Word2Vec\n",
    "\n",
    "### 📖 Concept\n",
    "\n",
    "**Average Word2Vec** is a simple yet effective way to represent an entire sentence or document as a single vector.\n",
    "\n",
    "* Each word in the sentence is represented by its **Word2Vec vector** (e.g., 300-dim).\n",
    "* The sentence vector is the **average of all word vectors** in that sentence.\n",
    "* This representation is then used as input for a **machine learning model** (e.g., Logistic Regression, SVM, Neural Network) for classification tasks like sentiment analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Example Dataset\n",
    "\n",
    "| Document | Text             | Sentiment (O/P) |\n",
    "| -------- | ---------------- | --------------- |\n",
    "| D1       | The food is good | 1 (Positive)    |\n",
    "| D2       | The food is bad  | 0 (Negative)    |\n",
    "| D3       | Pizza is amazing | 1 (Positive)    |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Step-by-Step Example Using Google’s Pretrained Word2Vec (300 Dimensions)\n",
    "\n",
    "**Input:** Google News Pretrained Word2Vec model (3 million words, 300-dimensional vectors)\n",
    "\n",
    "| Word | Embedding (300-d vector)       | Example (truncated) |\n",
    "| ---- | ------------------------------ | ------------------- |\n",
    "| The  | [0.1, 0.05, -0.02, … , 0.08]   | → 300 dims          |\n",
    "| food | [-0.03, 0.09, 0.15, … , -0.04] | → 300 dims          |\n",
    "| is   | [0.02, 0.01, 0.00, … , 0.05]   | → 300 dims          |\n",
    "| good | [0.21, 0.14, 0.06, … , 0.18]   | → 300 dims          |\n",
    "\n",
    "#### Step 1: Get each word’s vector from pretrained model\n",
    "\n",
    "* `v(The)` → 300-d vector\n",
    "* `v(food)` → 300-d vector\n",
    "* `v(is)` → 300-d vector\n",
    "* `v(good)` → 300-d vector\n",
    "\n",
    "#### Step 2: Compute Average Vector for the Sentence\n",
    "\n",
    "For sentence `D1: The food is good`:\n",
    "\n",
    "[\n",
    "V_{D1} = \\frac{v(The) + v(food) + v(is) + v(good)}{4}\n",
    "]\n",
    "\n",
    "Result: `V_D1` = a single **300-dimensional vector** representing the entire sentence.\n",
    "\n",
    "#### Step 3: Use Average Vector as Input to Classifier\n",
    "\n",
    "* `V_D1` → Input to ML model\n",
    "* Output label = `1` (Positive)\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Intuition\n",
    "\n",
    "Averaging word embeddings captures the **overall semantic meaning** of a sentence, smoothing out noise from individual words.\n",
    "\n",
    "* Works well for short texts.\n",
    "* Simpler than RNNs or Transformers.\n",
    "* Fast and effective for classical ML tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧰 Code Example (Using Gensim)\n",
    "\n",
    "```python\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "# Load Google's pretrained model (binary format)\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "def avg_word2vec(sentence, model):\n",
    "    words = [w for w in sentence.lower().split() if w in model]\n",
    "    if not words:\n",
    "        return np.zeros(model.vector_size)\n",
    "    vectors = [model[w] for w in words]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "sentences = [\"The food is good\", \"The food is bad\", \"Pizza is amazing\"]\n",
    "labels = [1, 0, 1]\n",
    "\n",
    "X = np.array([avg_word2vec(s, model) for s in sentences])\n",
    "print(X.shape)  # (3, 300)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Why Average Word2Vec?\n",
    "\n",
    "✅ **Advantages:**\n",
    "\n",
    "* Simple, fast, and effective for small datasets.\n",
    "* Captures semantic meaning through averaged vectors.\n",
    "* Fixed-length representation for variable-length sentences.\n",
    "\n",
    "❌ **Disadvantages:**\n",
    "\n",
    "* Loses word order information.\n",
    "* Contextual meaning is averaged out.\n",
    "* Not suitable for long or complex sentences.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Visualization Table Example\n",
    "\n",
    "Below is a visual representation of how each word vector contributes to the averaged embedding.\n",
    "\n",
    "| Word | Word2Vec Vector (300 dims) | → | Contribution to Average |\n",
    "| ---- | -------------------------- | - | ----------------------- |\n",
    "| The  | [x₁, x₂, …, x₃₀₀]          |   | ✔                       |\n",
    "| food | [y₁, y₂, …, y₃₀₀]          |   | ✔                       |\n",
    "| is   | [z₁, z₂, …, z₃₀₀]          |   | ✔                       |\n",
    "| good | [g₁, g₂, …, g₃₀₀]          |   | ✔                       |\n",
    "\n",
    "Final vector → `(v(The) + v(food) + v(is) + v(good)) / 4` → `[avg₁, avg₂, …, avg₃₀₀]`\n",
    "\n",
    "---\n",
    "\n",
    "## 🟩 Advantages of Word2Vec\n",
    "\n",
    "1. **Sparse → Dense Representation** – reduces memory & improves model performance.\n",
    "2. **Semantic Meaning Captured** – words like *good*, *honest*, *nice* cluster together.\n",
    "3. **Fixed-Dimension Vectors** – every word has equal-length representation (e.g., 300 dims).\n",
    "4. **Efficient Training** – negative sampling speeds up large-vocab training.\n",
    "5. **Transfer Learning Ready** – pretrained embeddings can be reused in various NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 🟥 Disadvantages of Word2Vec\n",
    "\n",
    "1. **OOV Problem** – cannot represent unseen words.\n",
    "2. **Context-Independent** – same vector for *“bank”* (river vs finance).\n",
    "3. **Requires Large Data** – poor results on small corpora.\n",
    "4. **Limited Interpretability** – embeddings are not human-readable.\n",
    "5. **Domain Dependence** – may not generalize across domains.\n",
    "\n",
    "---\n",
    "\n",
    "### 📘 Summary\n",
    "\n",
    "| Aspect              | Word2Vec                   | Average Word2Vec               |\n",
    "| ------------------- | -------------------------- | ------------------------------ |\n",
    "| **Output Type**     | Word embeddings            | Sentence embeddings            |\n",
    "| **Dimension**       | Fixed per word (e.g., 300) | Fixed per sentence (e.g., 300) |\n",
    "| **Handles OOV**     | ❌                          | ❌                              |\n",
    "| **Preserves Order** | ❌                          | ❌                              |\n",
    "| **Ease of Use**     | ⭐⭐⭐⭐                       | ⭐⭐⭐⭐⭐                          |\n",
    "\n",
    "---\n",
    "\n",
    "**Google Word2Vec Model:**\n",
    "\n",
    "* Trained on **3 billion words** from Google News.\n",
    "* **3 million vocabulary words**.\n",
    "* **300-dimensional feature vectors.**\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
